\documentclass{article}
%zbiór wzorów z wstecznej propagacji błędu
%http://www.r-bloggers.com/sweave-tutorial-1-using-sweave-r-and-make-to-generate-a-pdf-of-multiple-choice-questions/
\usepackage[table,dvipsnames]{xcolor}
%\usepackage[polish]{babel}
\usepackage{amsmath, amssymb, wasysym}
\usepackage{textcomp}
\usepackage{graphicx}           
%\usepackage{graphs}		% http://www8.cs.umu.se/~drewes/graphs/
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows,calc}
\usepackage[T1]{polski}                                           
\usepackage[T1]{fontenc}
% \usepackage[latin9]{inputenc}   
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{caption}          
\usepackage{rotating}      
\usepackage{color}
\usepackage{verbatim}
\usepackage{neuralnetwork}

\usepackage{tikz}

%\hypersetup{pdfborder=0 0 0}
\def\layersep{2.5cm}

%\addtolength{\belowcaptionskip}{0.2cm}

\renewcommand{\labelitemii}{$\diamond$}

\renewcommand{\captionfont}{\small\itshape}

\renewcommand{\familydefault}{\sfdefault}

\definecolor{red}{rgb}{0.4, 0.0, 0.0}
\definecolor{green}{rgb}{0.0, 0.4, 0.0}
\definecolor{blue}{rgb}{0.0, 0.0, 0.4}
\definecolor{magenta}{rgb}{0.4, 0.4, 0.0}
\definecolor{orange}{rgb}{0.2, 0.2, 0.0}


\usepackage[top=2cm, left=2cm, right=2cm, bottom=2cm]{geometry}

\tikzset{
block/.style={
  draw,
  rectangle, 
  text width=3em, 
  text centered, 
  minimum height=8mm,     
  node distance=2.3em
  }, 
line/.style={draw}
}


\begin{document}
\SweaveOpts{concordance=TRUE}


\begin{center}
{\LARGE Wsteczna propagacja błędu}
\end{center}



\begin{itemize}


\item[]
\underline{Informacje:}
Propagacja wsteczna – podstawowy algorytm uczenia nadzorowanego wielowarstwowych, jednokierunkowych sieci neuronowych. Podaje on przepis na zmianę wag dowolnych połączeń elementów przetwarzających rozmieszczonych w sąsiednich warstwach sieci. Oparty jest on na minimalizacji sumy kwadratów błędów (lub innej funkcji błędu) uczenia z wykorzystaniem optymalizacyjnej metody największego spadku. Dzięki zastosowaniu specyficznego sposobu propagowania błędów uczenia sieci powstałych na jej wyjściu, tj. przesyłania ich od warstwy wyjściowej do wejściowej, algorytm propagacji wstecznej stał się jednym z najskuteczniejszych algorytmów uczenia sieci
\url{https://pl.wikipedia.org/wiki/Propagacja_wsteczna}.

\item[]
\underline{Topologia sieci:}
\def\layersep{2.5cm}

\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:$x_\y$] (I-\name) at (0,-\y cm) {$\diagup$};

        \node[] (I-4) at (0,-4 cm) {$\vdots$};

        \node[input neuron, pin=left:$x_N$] (I-5) at (0,-5 cm) {$\diagup$};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {$\frac{S}{\sum}$};

    % Draw the output layer nodes
    \foreach \name / \y in {1,...,3}
%        \path[yshift=1.0cm]
	\node[output neuron,pin={[pin edge={->}]right:$y_\y$}, right of=H-3] (Y-\name) at (\layersep,-\y cm) {};

    \node[] (Y-4) at (2*\layersep,-4 cm) {$\vdots$};

    \node[output neuron,pin={[pin edge={->}]right:$y_M$}] (Y-5) at (2*\layersep,-5 cm) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge (H-\dest);

    \foreach \source in {5,...,5}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,5}
        \foreach \dest in {1,...,3,5}
	    \path (H-\source) edge (Y-\dest);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl) {Ukryta warstwa};
    \node[annot,left of=hl] {Wejściowa warstwa};
    \node[annot,right of=hl] {Wyjściowa warstwa};
\end{tikzpicture}


<<fig=false,width=4,height=4,echo=false>>=
pkglist<-c("clusterGeneration","corrplot","nnet","neuralnet","RSNNS","reshape","rockchalk","fifer","ade4","sqldf","plyr","dplyr")
pkgcheck <- pkglist %in% row.names(installed.packages())
for(i in pkglist[!pkgcheck]){install.packages(i,depend=TRUE)}
@
<<fig=true,width=4,height=4,echo=true>>=
#funkcja progowa sigmoidalna
sigmoid <- function(x) {
  1.0 / (1.0 + exp(-x))
}
#generuje x od -100 do 100 i dzielę przez 10, aby mieć od -10 do 10 z gęstszym upakowaniem 
x <- (c(1:200)-100)/10
plot(x,sigmoid(x))
@

\item[]
\underline{Funkcja błędu aproksymacji:}
\begin{align*}
F &= \sum_{p \in P} \ ^pE \ \ \ \text{, gdzie $p$ jest elementem w zbiorze trenującym $P$} \\
^p E &= \frac{1}{2} \sum_{j \in M} \left(\hat{y}_m - y_m \right)^2 \ \ \ \text{, gdzie $\hat{y}_m$ trenuje $m$ wyjście $y_m$}
\end{align*}

\item[]
\underline{Funkcje wag - współczynników połączeń między neuronami:}
\begin{align*}
^p \vartriangle w_{hm} &\sim - \nabla_w \cdot \ ^p E \ \ \ \text{, gdzie $w_{hm}$ jest wagą połączenia z neuronu $h$ do neuronu $m$} \\
\vartriangle w_{hm} &= - \eta \textcolor{red}{\frac{\partial E \left( w_{hm} \right) }{ \partial w_{hm}} } \ \ \ \text{, gdzie $\eta$ jest współczynnikiem uczenia się}
\end{align*}

\item[]
\underline{Dla wyjściowych neuronów z ostatniej warstwy:}

$net_m = \sum_{i=0}^H w_{im} \tilde{o}_i$

$o_m = y_m = f_m(net_m) $

$ \textcolor{red}{\frac{\partial E \left( w_{hm} \right) }{ \partial w_{hm} }} = \textcolor{magenta}{\frac{\partial E }{ \partial net_m }} \cdot \textcolor{green}{\frac{ \partial net_m }{ \partial w_{hm} }}$

\begin{align*}
\textcolor{green}{\frac{\partial net_m }{ \partial w_{hm} }} &= \frac{ \partial }{ \partial w_{hm} } \cdot net_m
= \frac{ \partial }{ \partial w_{hm} } \sum_{i=0}^H w_{im} \tilde{o}_i 
= \sum_{i=0}^H \frac{ \partial }{ \partial w_{hm} } w_{im} \tilde{o}_i 
= \frac{ \partial }{ \partial w_{hm} } \tilde{o}_h w_{hm} 
= \text{\textcolor{green}{\framebox{$ \tilde{o}_h $}}} \\
\\
\textcolor{magenta}{\frac{\partial E }{ \partial net_m }} &= \frac{ \partial E }{ \partial y_m } \cdot \textcolor{orange}{\frac{ \partial y_m }{ \partial net_m }} \ \left( = \textcolor{magenta}{- \delta_m} \right) 
= \frac{ \partial E }{ \partial y_m } \cdot \frac{ \partial }{ \partial net_m } f_m(net_m) 
= \underbrace{\textcolor{blue}{\frac{ \partial E }{ \partial y_m }} \cdot \text{\textcolor{orange}{\framebox{$ f'_m (net_m) $}}}}_{=: -\delta_m} \\
\\
\textcolor{blue}{\frac{ \partial E }{ \partial y_m }} &= \frac{ \partial }{ \partial y_m } \cdot \frac{1}{2} \sum_{j=1}^M \left( \hat{y}_j - y_j \right)^2 
= \text{\textcolor{blue}{\framebox{$ - (\hat{y}_m - y_m) $}}} \\
\end{align*}

\begin{align*}
\vartriangle w_{hm} &= - \eta \textcolor{red}{\frac{ \partial E }{ \partial w_{hm} }} 
= \textcolor{blue}{-} \eta \textcolor{blue}{\frac{ \partial E }{ \partial y_m }} \textcolor{orange}{\frac{ \partial y_m}{ \partial net_m}} \textcolor{green}{\frac{ \partial net_m}{ \partial w_{hm}}} 
= \eta \textcolor{blue}{\left( \hat{y}_m - y_m \right)} \textcolor{orange}{f'_m(net_m)} \textcolor{green}{\tilde{o}_h}
\end{align*}

\framebox{$\delta_m = \textcolor{blue}{\left( \hat{y}_m - y_m \right)} \cdot \textcolor{orange}{f'_m(net_m)}$}

\framebox{$ \vartriangle w_{hm} = \eta \cdot \delta_m \cdot \tilde{o}_h $}
Widrow-Hoff reguła / $\delta$-reguła
\\

\item[]
\underline{DLa neuronów z ukrytych warstw między wejściem, a wyjściem:}

$net_h = \sum_{i=0}^H w_{ih} \tilde{o}_i$

$ \textcolor{red}{\frac{\partial E }{ \partial w_{kh} }} = \textcolor{magenta}{\frac{\partial E }{ \partial net_h }} \cdot \textcolor{green}{\frac{ \partial net_h }{ \partial w_{kh} } }$

\begin{align*}
\delta_h &= - \textcolor{magenta}{\frac{ \partial E }{ \partial net_h}} 
= \textcolor{blue}{- \frac{ \partial E }{ \partial o_h}} \cdot \textcolor{orange}{\frac{ \partial o_h }{ \partial net_h}} \\
\\
\textcolor{blue}{- \frac{ \partial E }{ \partial o_h}} &= - \frac{ \partial E \left( \underline{net}_{l=1} , \underline{net}_{l=2} , \hdots , \underline{net}_{l=L} \right) }{ \partial o_h} 
= \sum_{l=1}^L \left( - \frac{ \partial E}{ \partial \underline{net}_l } \right) \cdot \frac{ \partial \underline{net}_l }{ \partial o_h} 
= \sum_{l=1}^L \underline{\delta}_l \cdot \frac{ \partial }{ \partial o_h} \sum_{j=0}^H \underline{w}_{jl} \cdot o_j 
= \textcolor{blue}{\sum_{l=1}^L \underline{\delta}_l \cdot \underline{w}_{hl}}
\end{align*}

\framebox{$\delta_h = \textcolor{blue}{\sum_{l=1}^L \left( \underline{\delta}_l \cdot \underline{w}_{hl} \right)} \cdot \textcolor{orange}{f'\left( net_h \right)}$}

\framebox{$ \vartriangle w_{kh} = \eta \cdot \delta_h \cdot \tilde{o}_k $}

\end{itemize}


\begin{tikzpicture}[
    plain/.style={
      draw=none,
      fill=none,
      },
    net/.style={
      matrix of nodes,
     nodes={
       draw,
        circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  }, 
 >=latex
 ]
\matrix[net] (mat)
{ 
|[plain]| \parbox{1cm}{\centering Wejściowa\\warstwa} & |[plain]| \parbox{1cm}{\centering     Ukryta\\warstwa} & 
|[plain]| \parbox{1cm}{\centering Wyjściowa\\warstwa} \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
|[plain]| & |[plain]| \\
& & \\
|[plain]| & |[plain]| \\
& |[plain]| \\
|[plain]| & \\
& |[plain]| \\
};
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {Wejście \mi} +(-2cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
%\draw[->] (mat-6-3) -- node[above] {Ouput} +(2cm,0);
\path [line] node{neuron} -- (mat-1-1);
\draw[->] (mat-6-3) -- ++(0pt,3cm) -| node[pos=0.15,above] {Błąd wstecznej propagacji jako korekta wag} ( $ (mat-2-1)!0.5!(mat-2-2) $ );
\end{tikzpicture}

\begin{tikzpicture}
    %%Create a style for the arrows we are using
    \tikzset{normal arrow/.style={draw, thin}}
    %%Create the different coordinates to place the nodes
    \path (0,0) coordinate (1) ++(0,-2) coordinate (2) ++(0,-2) coordinate (3);
    \path (1) ++(-3,-.2) coordinate (x1);
    \path (3) ++(-3, .2) coordinate (x2);
    %%Use the calc library and partway modifiers to generate the second and third level points
    \path ($(1)!.5!(2)!3 cm!90:(2)$) coordinate (4);
    \path ($(2)!.5!(3)!3 cm!90:(3)$) coordinate (5);
    \path ($(4)!.5!(5)!3 cm!90:(5)$) coordinate (6);
    \path (6) ++(3,0) coordinate (7);
    %%Place nodes at each point using the foreach construct
    \foreach \i/\color in 
{1/Magenta!60,2/MidnightBlue!60,3/CadetBlue!80,4/CadetBlue!80,5/CadetBlue!80,6/CadetBlue!80}{
      \node[draw,circle,shading=axis,top color=\color, bottom color=\color!black,shading angle=45] (n\i) 
at (\i) {$f_{\i}(e)$};
    }
    %%Place the remaining nodes separately
    \node (nx1) at (x1) {$\mathbf{x_1}$};
    \node (nx2) at (x2) {$\mathbf{x_2}$};
    \node (ny)  at (7)  {$\mathbf{y}$};
    %%Drawing the arrows
    \path[normal arrow] (nx1) -- (n1);
    \path[normal arrow] (nx1) -- (n3);
    \path[normal arrow] (nx2) -- (n1);
    \path[normal arrow] (nx2) -- (n3);
    \path[normal arrow] (n1)  -- (n4);
    \path[normal arrow] (n1)  -- (n5);
    \path[normal arrow] (n2)  -- (n4);
    \path[normal arrow] (n2)  -- (n5);
    \path[normal arrow] (n3)  -- (n4);
    \path[normal arrow] (n3)  -- (n5);
    \path[normal arrow] (n4)  -- (n6);
    \path[normal arrow] (n5)  -- (n6);
    \path[normal arrow] (n6)  -- (ny);
    %%Drawing the cyan arrows including the labels
    \path[normal arrow,Cyan] (nx1) -- node[above=.5em,Cyan] {$\mathbf{w_{(x1)2}}$} (n2);
    \path[normal arrow,Cyan] (nx2) -- node[below=.5em,Cyan] {$\mathbf{w_{(x2)2}}$} (n2);
  \end{tikzpicture}

\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Wejście \#\y] (I-\name) at (0,-\y) {};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Wyjście}, right of=H-3] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,5}
        \path (H-\source) edge (O);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl) {Ukryta warstwa};
    \node[annot,left of=hl] {Wejściowa warstwa};
    \node[annot,right of=hl] {Wyjściowa warstwa};
\end{tikzpicture}

    \begin{neuralnetwork}[height=2.5, layertitleheight=0, nodespacing=2.8cm, layerspacing=1.7cm]
        \newcommand{\nodetextclear}[2]{}
        \newcommand{\nodetextxnb}[2]{\ifnum0=#2 \else $x_#2$ \fi}
        \newcommand{\logiclabel}[1]{\,{$\scriptstyle#1$}\,}
        \newcommand{\nodetextY}[2]{$y$}
        \newcommand{\linklabelsU}[4]{\logiclabel{+1}}
        \newcommand{\linklabelsA}[4]{\ifnum0=#2 \logiclabel{+3} \else \logiclabel{-2} \fi}
        \setdefaultnodetext{\nodetextclear}
        % Input layer
        \inputlayer[count=2, bias=false, text=\nodetextxnb]
        % links to first hidden layer from input layer
        \hiddenlayer[count=3, bias=false, exclude={1, 3}]
            \link[from layer=0, to layer=1, from node=1, to node=2, label=\linklabelsA]
            \link[from layer=0, to layer=1, from node=2, to node=2, label=\linklabelsA]
        \hiddenlayer[count=2, bias=true, biaspos=center]
        % links to second hidden layer from input and first hidden layer
            \link[from layer=0, to layer=2, from node=1, to node=1, label=\linklabelsA]
            \link[from layer=1, to layer=2, from node=2, to node=1, label=\linklabelsA]
            \link[from layer=1, to layer=2, from node=2, to node=2, label=\linklabelsA]
            \link[from layer=0, to layer=2, from node=2, to node=2, label=\linklabelsA]
        \outputlayer[count=1, text=\nodetextY]
        % links to output layer from second hidden layer
            \link[from layer=2, to layer=3, from node=1, to node=1, label=\linklabelsA]
            \link[from layer=2, to layer=3, from node=2, to node=1, label=\linklabelsA]
        % links from bias node
            \link[from layer=2, to layer=1, from node=0, to node=2, label=\linklabelsA]
            \link[from layer=2, to layer=2, from node=0, to node=1, label=\linklabelsA]
            \link[from layer=2, to layer=2, from node=0, to node=2, label=\linklabelsA]
            \link[from layer=2, to layer=3, from node=0, to node=1, label=\linklabelsA]
    \end{neuralnetwork}



\end{document}